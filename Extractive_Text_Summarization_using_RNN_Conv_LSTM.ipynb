{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoSTuU34mH_F"
      },
      "source": [
        "# Suppose we are NLP Engineer working to build an automatic text summarizer for a News Channel. The news channel uses flashcards of broad news articles to design the front page of their blog, which is read by more than a million readers across the globe. To develop the content for these flashcards, the news channel editors manually summarize the prospective news articles. This process is very time-consuming therefore, it becomes important to build a text summarizer that can automatically generate summaries.\n",
        "\n",
        "# The text summarizer developed by us is going to play a crucial role in reducing the turnaround time for the news editors in developing the content for the flashcards. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAAFAKMipiqx"
      },
      "source": [
        "# The dataset which we will be using to train our text summarizer, is DeepMind Q and A dataset. This dataset is discussed in the following research publication: \n",
        "\n",
        "https://arxiv.org/abs/1506.03340\n",
        "\n",
        "# This dataset primarily contains the documents and accompanying questions from the news articles of the CNN news channel. The url of the dataset is here: \n",
        "\n",
        "https://cs.nyu.edu/~kcho/DMQA/\n",
        "\n",
        "# Within the CNN column, we will be using the section named \"Stories\" to fetch our dataset which will be having long paragraphs as well as their summaries. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA04TiNKrJUn"
      },
      "source": [
        "# We have already downloaded the dataset from the above url, now let's unzip it: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHhasp7Sl0h9"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuQLDNPsrmGv"
      },
      "outputs": [],
      "source": [
        "! tar -xvf /content/drive/MyDrive/cnn_stories.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYScn7gkrqrx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np \n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06VgcCVHyUZc"
      },
      "source": [
        "# Let's create functions to load the dataset and split stories into news paragraphs as well as summaries or highlights. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MG-3iedw9-n"
      },
      "outputs": [],
      "source": [
        "def load_story(single_story_path):\n",
        "\n",
        "  file_handle = open(single_story_path,encoding=\"utf-8\")\n",
        "  single_complete_story = file_handle.read()\n",
        "  file_handle.close()\n",
        "  return single_complete_story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z41K-Mp_zCQ8"
      },
      "outputs": [],
      "source": [
        "def split_story_into_para_highlights(single_complete_story):\n",
        "\n",
        "  highlight_loc = single_complete_story.find(\"@highlight\")\n",
        "  para, highlights = single_complete_story[:highlight_loc], single_complete_story[highlight_loc:].split(\"@highlight\")\n",
        "  highlights = [summary.strip() for summary in highlights if len(highlights) > 0]\n",
        "\n",
        "  return para,highlights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjy8qWjS1Q_y"
      },
      "outputs": [],
      "source": [
        "paragraphs = list()\n",
        "summaries = list()\n",
        "\n",
        "for story_filename in os.listdir(\"./cnn/stories\"):\n",
        "\n",
        "  single_story_path = os.path.join(\"./cnn/stories\",story_filename)\n",
        "  single_complete_story = load_story(single_story_path)\n",
        "\n",
        "  para, highlights = split_story_into_para_highlights(single_complete_story)\n",
        "\n",
        "  paragraphs.append(para)\n",
        "  summaries.append(highlights)\n",
        "\n",
        "stories = dict(zip([\"Story_paragraphs\",\"Abstractive_summaries\"],[paragraphs,summaries]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdlJv1rQLnK_"
      },
      "source": [
        "# We have now converted our data into abstractive text summarization dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwMw_2xD4VsS"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrHqPs7GAsIZ"
      },
      "outputs": [],
      "source": [
        "pkl_file_handle = open(\"cnn_news_stories.pkl\",\"wb\")\n",
        "pickle.dump(stories,pkl_file_handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rFpJvo9DnHs"
      },
      "outputs": [],
      "source": [
        "stories = pickle.load(open(\"./cnn_news_stories.pkl\",\"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSeYuBZvNhxn"
      },
      "outputs": [],
      "source": [
        "stories[\"Story_paragraphs\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6luBPBAa5-J"
      },
      "source": [
        "# Lets create a small function to preprocess each line of paragraphs as well as abstractive summaries. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEZ_xuzdc1PL"
      },
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLn6tEO4Zy8C"
      },
      "outputs": [],
      "source": [
        "def preprocess_single_sent_per_story(sents_per_story):\n",
        "\n",
        "  cleaned_sents = list()\n",
        "  waste_tokens_ascii_values_mapping = dict(zip(list(range(33,48)) + list(range(58,65)) +\\\n",
        "                                                 list(range(91,97)) + list(range(123,127)),[None]*32))\n",
        "  for sent in sents_per_story:\n",
        "            \n",
        "    loc = sent.find('(CNN) -- ')\n",
        "    if loc > -1:\n",
        "      sent = sent[loc+len('(CNN)'):]\n",
        "        \n",
        "    sent = sent.split()\n",
        "    sent = [token.lower() for token in sent]\n",
        "    sent = [token.translate(waste_tokens_ascii_values_mapping) for token in sent]\n",
        "    sent = [token for token in sent if token.isalpha()]\n",
        "    cleaned_sents.append(' '.join(sent))\n",
        "    \n",
        "  cleaned_sents = [sent for sent in cleaned_sents if len(sent) > 0]\n",
        "  return cleaned_sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oj3DVwPbe5p"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-3fWqvplE8W"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(range(len(stories[\"Story_paragraphs\"]))):\n",
        "\n",
        "  stories[\"Story_paragraphs\"][i] = preprocess_single_sent_per_story(stories[\"Story_paragraphs\"][i].split(\"\\n\"))\n",
        "  stories[\"Abstractive_summaries\"][i] = preprocess_single_sent_per_story(stories[\"Abstractive_summaries\"][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3FZxy_Bg1ex"
      },
      "outputs": [],
      "source": [
        "stories[\"Story_paragraphs\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buQEykrskdfG"
      },
      "outputs": [],
      "source": [
        "stories[\"Abstractive_summaries\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikEvTDlHtYIS"
      },
      "source": [
        "# Now, we will be making each story paragraphs as short and as concise as possible. The way to do this is by using ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score. We will be here using ROUGE score to extract the most relevant sentences from each story paragraph based on the given summary corresponding to that story paragraph. \n",
        "\n",
        "# So, basically what we will be doing is that for each story, we will be calculating ROUGE score between each sentence of the story paragraph and each of the summaries corresponsing to that specific paragraph. Furthermore, we will be selecting top 5 sentences from the story paragraphs with respect to their ROUGE score. \n",
        "\n",
        "# In this manner, we will be making the story paragraphs concise. \n",
        "\n",
        "# To know more about ROUGE score and what extractive summarization is, please navigate to this url: \n",
        "\n",
        "https://arxiv.org/abs/1807.02305"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtB0dBZbo-4T"
      },
      "outputs": [],
      "source": [
        "! pip install Rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBnYlHzQyAzR"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B744-jNdyNeG"
      },
      "outputs": [],
      "source": [
        "rouge_obj = Rouge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwgp2eIryVeq"
      },
      "outputs": [],
      "source": [
        "def compute_rouge_score(story_para_sent, abstractive_summaries):\n",
        "\n",
        "  score_per_story_para_sent = list()\n",
        "\n",
        "  for summary in abstractive_summaries:\n",
        "\n",
        "    summary_scores = rouge_obj.get_scores(summary, story_para_sent)\n",
        "    score_per_story_para_sent.append(summary_scores[0]['rouge-1']['f'])\n",
        "    \n",
        "  return max(score_per_story_para_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJuDUHvj2g_D"
      },
      "outputs": [],
      "source": [
        "def fetch_each_story_top5_para_sents(story_para, abstractive_summaries):\n",
        "\n",
        "  story_para_sents = list()\n",
        "  max_scores = list()\n",
        "\n",
        "  for i in range(0, len(story_para)):\n",
        "\n",
        "    story_para_sent = story_para[i]\n",
        "    max_score = compute_rouge_score(story_para_sent, abstractive_summaries)\n",
        "\n",
        "    story_para_sents.append(story_para_sent)\n",
        "    max_scores.append(max_score)\n",
        "        \n",
        "  story_para_sents = np.array(story_para_sents)\n",
        "    \n",
        "  max_scores1 = np.array(max_scores)\n",
        "  max_scores2 = np.sort(max_scores)[::-1]\n",
        "  idx = np.argsort(max_scores)[::-1]\n",
        "     \n",
        "  idx = idx[0:5]\n",
        "    \n",
        "  return list(story_para_sents[idx]), max_scores2[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX6wtKnBk2Dz"
      },
      "outputs": [],
      "source": [
        "fetch_each_story_top5_para_sents(stories[\"Story_paragraphs\"][0],stories[\"Abstractive_summaries\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWvAQVP11e-B"
      },
      "outputs": [],
      "source": [
        "len(stories[\"Story_paragraphs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ouWliU5CGZe"
      },
      "outputs": [],
      "source": [
        "all_stories_top5_sents_dict = dict()\n",
        "all_stories_top5_sents_scores = dict()\n",
        "\n",
        "for story_idx in tqdm(range(0, len(stories[\"Story_paragraphs\"]))):\n",
        "    \n",
        "  story_para_sents = stories[\"Story_paragraphs\"][story_idx]\n",
        "  abstractive_summaries = stories[\"Abstractive_summaries\"][story_idx]\n",
        "  top5_para_sents, top5_sents_scores = fetch_each_story_top5_para_sents(story_para_sents,abstractive_summaries)\n",
        "  all_stories_top5_sents_dict[story_idx] = top5_para_sents\n",
        "  all_stories_top5_sents_scores[story_idx] = top5_sents_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWphn2aOJ1HH"
      },
      "outputs": [],
      "source": [
        "pkl_file_handle = open(\"./all_stories_top5_sents_dict.pkl\",\"wb\")\n",
        "pickle.dump(all_stories_top5_sents_dict,pkl_file_handle)\n",
        "\n",
        "pkl_file_handle = open(\"./all_stories_top5_sents_scores.pkl\",\"wb\")\n",
        "pickle.dump(all_stories_top5_sents_scores,pkl_file_handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uutoBxrOGkR4"
      },
      "outputs": [],
      "source": [
        "all_stories_top5_sents_dict = pickle.load(open(\"./all_stories_top5_sents_dict.pkl\",\"rb\"))\n",
        "all_stories_top5_sents_scores = pickle.load(open(\"./all_stories_top5_sents_scores.pkl\",\"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMYxcoAxdfU2"
      },
      "source": [
        "# Let's now create a Pandas DataFrame where each row will consist of story index (story_idx), sentence index of top 5 sentences selected from the story paragraph (sent_idx), Each sentence out of top 5 sentences in a story paragraph, label representing whether each sentence out of top 5 sentences is in the extractive summary or not (extractive_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEX8vtmu1SKi"
      },
      "outputs": [],
      "source": [
        "len(stories[\"Story_paragraphs\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjZEdx3JdQ-3"
      },
      "outputs": [],
      "source": [
        "story_idx = list()\n",
        "sent_idx = list()\n",
        "sents_list = list()\n",
        "extractive_label = list()\n",
        "\n",
        "for i in tqdm(range(0, len(stories[\"Story_paragraphs\"]))):\n",
        "    \n",
        "  top5_para_sents = all_stories_top5_sents_dict[i]\n",
        "    \n",
        "  for j, para_sent in enumerate(stories[\"Story_paragraphs\"][i]):\n",
        "        \n",
        "    ohe_label =  int(para_sent in top5_para_sents)\n",
        "    extractive_label.append(ohe_label)\n",
        "    sents_list.append(para_sent)\n",
        "    sent_idx.append(j)\n",
        "    story_idx.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY8c-0s1jj4o"
      },
      "outputs": [],
      "source": [
        "extractive_summaries_df = pd.DataFrame()\n",
        "extractive_summaries_df[\"Story_idx\"] = story_idx\n",
        "extractive_summaries_df[\"Sent_idx\"] = sent_idx\n",
        "extractive_summaries_df[\"Para_sents\"] = sents_list\n",
        "extractive_summaries_df[\"Extractive_label\"] = extractive_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jex9dpGk78H"
      },
      "outputs": [],
      "source": [
        "extractive_summaries_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQvtDGFh2Nka"
      },
      "outputs": [],
      "source": [
        "len(extractive_summaries_df[\"Story_idx\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UMY5V5HlEUL"
      },
      "outputs": [],
      "source": [
        "extractive_summaries_df.to_pickle(\"extractive_summaries_df.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13Ke8Ao04Hnl"
      },
      "outputs": [],
      "source": [
        "data = pd.read_pickle(\"extractive_summaries_df.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "534ZaIUBRY_u"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKQO8bxg2oCA"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0SWo_4iiDb1"
      },
      "source": [
        "# Let's divide our data into Training, Cross Validation and Testing Data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUfSeP8aDfp0"
      },
      "outputs": [],
      "source": [
        "data_story_sents_count = data.groupby(\"Story_idx\").size().reset_index(name=\"Sentences_count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqDfkFZQFmN1"
      },
      "outputs": [],
      "source": [
        "data_story_sents_count.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUJI0BjyFv0a"
      },
      "outputs": [],
      "source": [
        "selected_stories_idx = list(data_story_sents_count[data_story_sents_count[\"Sentences_count\"] <= 20][\"Story_idx\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhoTKBtRGdGN"
      },
      "outputs": [],
      "source": [
        "len(selected_stories_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L29vJODJTADu"
      },
      "outputs": [],
      "source": [
        "train_story_ids = selected_stories_idx[:30000]\n",
        "cv_story_ids = selected_stories_idx[30000:40000]\n",
        "test_story_ids = selected_stories_idx[40000:]\n",
        "\n",
        "training_data = data[data[\"Story_idx\"].isin(train_story_ids)]\n",
        "cv_data = data[data[\"Story_idx\"].isin(cv_story_ids)]\n",
        "testing_data = data[data[\"Story_idx\"].isin(test_story_ids)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfbfefIQ1G78"
      },
      "outputs": [],
      "source": [
        "selected_stories_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHbYoBkkHiAH"
      },
      "outputs": [],
      "source": [
        "len(training_data[\"Story_idx\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwUXBKgp05Xs"
      },
      "outputs": [],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVNa-vdnHpHr"
      },
      "outputs": [],
      "source": [
        "len(cv_data[\"Story_idx\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-f3UmaTO08p2"
      },
      "outputs": [],
      "source": [
        "cv_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xil6c87iWQA"
      },
      "source": [
        "# Now, lets compute maximum number of sentences which a paragraph can have inside a story in a training data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IhFgcKua4Cd"
      },
      "outputs": [],
      "source": [
        "training_data = training_data.sort_values([\"Story_idx\",\"Sent_idx\"])\n",
        "sents_count = training_data.groupby(\"Story_idx\").size().reset_index(name=\"Sentences_count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40O800bYbcbh"
      },
      "outputs": [],
      "source": [
        "sents_count[\"Sentences_count\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEJGDyhWbm5m"
      },
      "outputs": [],
      "source": [
        "story_max_length = sents_count[\"Sentences_count\"].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB18aWF3b15H"
      },
      "outputs": [],
      "source": [
        "story_max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFNeeO9gcDle"
      },
      "outputs": [],
      "source": [
        "unique_sents = set(training_data[\"Para_sents\"].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2kVlraLcme5"
      },
      "outputs": [],
      "source": [
        "len(unique_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xIpVNdmdGBh"
      },
      "outputs": [],
      "source": [
        "num_labels = len(training_data[\"Extractive_label\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5h69Mw4373F"
      },
      "outputs": [],
      "source": [
        "num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk6EKsJY5IOv"
      },
      "outputs": [],
      "source": [
        "np.sort(training_data[\"Extractive_label\"].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u29FVIYzdW9D"
      },
      "outputs": [],
      "source": [
        "labels2idx = {l: i+1 for i,l in enumerate(np.sort(training_data[\"Extractive_label\"].unique()))}\n",
        "labels2idx[\"PAD\"] = 0\n",
        "idx2labels = {i: l for l,i in labels2idx.items()}\n",
        "print(labels2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QolTMI62irZr"
      },
      "source": [
        "# Let's now add two more columns into the Training, Cross Validation as well as Testing Data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8hRwEyb57wT"
      },
      "outputs": [],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D94584OBeqPQ"
      },
      "outputs": [],
      "source": [
        "def create_token_count_list(df):\n",
        "  \n",
        "  df['Number_tokens'] = df[\"Para_sents\"].apply(lambda x: len(x.split()))\n",
        "  df['Tokens_list'] = df[\"Para_sents\"].apply(lambda x: x.split())\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtIB7DmPgl4L"
      },
      "outputs": [],
      "source": [
        "training_data = create_token_count_list(training_data)\n",
        "cv_data = create_token_count_list(cv_data)\n",
        "testing_data = create_token_count_list(testing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ02SWsKhT-k"
      },
      "outputs": [],
      "source": [
        "training_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-o4fF21jECQ"
      },
      "source": [
        "# Now, let's compute the total number of unique tokens inside the training data paragraphs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m2pWMfNjziy"
      },
      "outputs": [],
      "source": [
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8JzKGrPhXDF"
      },
      "outputs": [],
      "source": [
        "total_unique_tokens = set(list(chain(*training_data['Tokens_list'].tolist())))\n",
        "num_unique_tokens = len(total_unique_tokens)\n",
        "\n",
        "token2idx = {token: i+2 for i,token in enumerate(total_unique_tokens)}\n",
        "token2idx[\"UNK\"] = 1\n",
        "token2idx[\"PAD\"] = 0\n",
        "\n",
        "idx2token = {i: token for token, i in token2idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1S0m8Qy7KRd"
      },
      "outputs": [],
      "source": [
        "len(idx2token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEahX2FSmX1T"
      },
      "outputs": [],
      "source": [
        "def create_sent_label_example(df):\n",
        "\n",
        "  df[\"Sent_example\"] = df[[\"Para_sents\",\"Extractive_label\"]].apply(tuple,axis=1)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfjonSlEp_8J"
      },
      "outputs": [],
      "source": [
        "training_data = create_sent_label_example(training_data)\n",
        "cv_data = create_sent_label_example(cv_data)\n",
        "testing_data = create_sent_label_example(testing_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zPWA-Wn71OD"
      },
      "outputs": [],
      "source": [
        "training_data.iloc[0][\"Sent_example\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C68WnJgcqqXf"
      },
      "outputs": [],
      "source": [
        "max_sent_length = 40\n",
        "\n",
        "def stories_representation(df):\n",
        "   \n",
        "  story_ids = df['Story_idx'].unique()\n",
        "  stories_examples = list()\n",
        "\n",
        "  for story_idx in tqdm(story_ids):\n",
        "\n",
        "    temp_story = list(df[df['Story_idx'] == story_idx][\"Sent_example\"])\n",
        "    stories_examples.append(temp_story)\n",
        "    X_token = np.zeros((len(stories_examples), story_max_length, max_sent_length))\n",
        "    \n",
        "    for idx, story_example in enumerate(stories_examples):\n",
        "\n",
        "      story_seq = list()\n",
        "        \n",
        "      # to give an upper bound on the maximum length of the token sequence for sentence\n",
        "      for i in range(story_max_length):\n",
        "\n",
        "          sent_seq = list()\n",
        "            \n",
        "          # to give an upper bound on the maximum length of tokens to consider\n",
        "          for j in range(max_sent_length):\n",
        "\n",
        "            try:\n",
        "                split_sent = story_example[i][0].split()\n",
        "                sent_seq.append(token2idx.get(split_sent[j]))\n",
        "            except:  \n",
        "                # exception will be there when there will not be any sentence for the length \n",
        "                # and will be padded 0\n",
        "                sent_seq.append(token2idx.get(\"PAD\"))\n",
        "\n",
        "          story_seq.append(sent_seq)\n",
        "        \n",
        "      X_token[idx] = np.array(story_seq)\n",
        "\n",
        "  return (X_token, stories_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGcZnI9Ptz95"
      },
      "outputs": [],
      "source": [
        "X_train,Y_train = stories_representation(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvksBYlRukvz"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRgFHv-sum9U"
      },
      "outputs": [],
      "source": [
        "X_cv,Y_cv = stories_representation(cv_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APAX-QzEusEY"
      },
      "outputs": [],
      "source": [
        "X_cv.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_deWPNOwVTY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn7W_BwNutpX"
      },
      "outputs": [],
      "source": [
        "def prepare_labels(story_examples):\n",
        "\n",
        "    Y = [[labels2idx[ex_content[1]] for ex_content in sent_example] for sent_example in story_examples]\n",
        "    Y = pad_sequences(maxlen=story_max_length, sequences=Y, value=labels2idx[\"PAD\"], padding='post', truncating='post')\n",
        "    Y = Y.reshape(-1, story_max_length, 1)\n",
        "    \n",
        "    return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDbL8yeCzJSA"
      },
      "outputs": [],
      "source": [
        "train_labels = prepare_labels(Y_train)\n",
        "cv_labels = prepare_labels(Y_cv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rR3BeEOv0RqH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmSz7hom0CnO"
      },
      "outputs": [],
      "source": [
        "training_data_batch_gen = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "training_data_batch_gen = (training_data_batch_gen.batch(64).cache().prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "cv_data_batch_gen = tf.data.Dataset.from_tensor_slices((X_cv, Y_cv))\n",
        "cv_data_batch_gen = (cv_data_batch_gen.batch(64).cache().prefetch(tf.data.experimental.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atVYfOsiNRAH"
      },
      "outputs": [],
      "source": [
        "! wget https://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt4J_z5aSG5N"
      },
      "outputs": [],
      "source": [
        "! unzip /content/drive/MyDrive/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHOsfWPDSI8P"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(token_idxes, embedding_path, topic_vector_dim):\n",
        "\n",
        "  embedding_matrix_dict = dict()\n",
        "\n",
        "  with open(embedding_path) as file_handle:\n",
        "\n",
        "    for line in file_handle:\n",
        "\n",
        "      values = line.split()\n",
        "      token = values[0]\n",
        "      topic_vector = np.asarray(values[1:], dtype='float32')\n",
        "      embedding_matrix_dict[token] = topic_vector\n",
        "\n",
        "  num_words = len(token_idxes) \n",
        "  embedding_matrix = np.zeros((num_words, topic_vector_dim))\n",
        "\n",
        "  for token, idx in token_idxes.items():\n",
        "\n",
        "    topic_vector = embedding_matrix_dict.get(token)\n",
        "\n",
        "    if topic_vector is not None:\n",
        "      embedding_matrix[idx] = topic_vector\n",
        "  \n",
        "  return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJtiSeAjYJyw"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, TimeDistributed, Embedding, Convolution1D, Dense, Flatten, Activation, RepeatVector, Permute, multiply\n",
        "from tensorflow.keras.layers import Lambda, Bidirectional, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP-RI0WMWb1q"
      },
      "outputs": [],
      "source": [
        "embedding_matrix_txt_path = \"/content/drive/MyDrive/glove.6B.100d.txt\"\n",
        "topic_vectors_dim = 100\n",
        "\n",
        "def text_summarization_model():\n",
        "\n",
        "  token_input = Input(shape=(story_max_length, max_sent_length,))\n",
        "  embedding_layer_out = TimeDistributed(Embedding(input_dim=(num_unique_tokens + 2), output_dim=topic_vectors_dim, input_length=max_sent_length,\n",
        "                                      weights=[create_embedding_matrix(token2idx, embedding_matrix_txt_path, topic_vectors_dim)], trainable=True))(token_input)\n",
        "\n",
        "  embedding_layer2_out = TimeDistributed(Convolution1D(32, 2, activation='relu',padding= 'same'))(embedding_layer_out)\n",
        "    \n",
        "  hidden_layer_out = TimeDistributed(Dense(1, activation='tanh'))(embedding_layer2_out)\n",
        "  hidden_layer_out = TimeDistributed(Flatten())(hidden_layer_out)\n",
        "  hidden_layer_out = TimeDistributed(Activation('softmax'))(hidden_layer_out)\n",
        "  hidden_layer_out = TimeDistributed(RepeatVector(32))(hidden_layer_out)\n",
        "  hidden_layer_out = TimeDistributed(Permute([2, 1]))(hidden_layer_out)\n",
        "  hidden_layer_out = multiply([embedding_layer2_out,hidden_layer_out])\n",
        "    \n",
        "  sent_embedding = TimeDistributed(Lambda(lambda x: K.sum(x, axis=-2)))(hidden_layer_out)\n",
        "    \n",
        "  lstm_nw = Bidirectional(LSTM(units=16, return_sequences=True))(sent_embedding)\n",
        "  nw_final_output = TimeDistributed(Dense(num_labels + 1, activation='softmax'))(lstm_nw)\n",
        "\n",
        "  model = Model([token_input], nw_final_output)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Agk3aFHFkeK4"
      },
      "outputs": [],
      "source": [
        "model = text_summarization_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_IYMJjHk9Mw"
      },
      "outputs": [],
      "source": [
        "lr_start = 1e-5\n",
        "lr_max = 1e-3\n",
        "lr_rampup_epochs = 5\n",
        "lr_to_sustain_epochs = 0\n",
        "lr_step_decay = 0.75"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUfBYEf3luRq"
      },
      "outputs": [],
      "source": [
        "def lr_scheduler(epoch):\n",
        "\n",
        "  if epoch < lr_rampup_epochs:\n",
        "    lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
        "\n",
        "  elif epoch < lr_rampup_epochs + lr_to_sustain_epochs:\n",
        "    lr = lr_max\n",
        "\n",
        "  else:\n",
        "    lr = lr_max * lr_step_decay**((epoch - lr_rampup_epochs - lr_to_sustain_epochs)//10)\n",
        "\n",
        "  return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7biaSDYXmuWd"
      },
      "outputs": [],
      "source": [
        "lr_scheduler_cb = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=True)\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dO0RkHqsnAAA"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizer.Adam(lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nVBqNi5nQM3"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4z2WrqTneqc"
      },
      "outputs": [],
      "source": [
        "model.fit(training_data_batch_gen, validation_data=cv_data_batch_gen,epochs=50,callbacks=[lr_scheduler_cb, early_stopping_cb], verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvp-kF4GodLc"
      },
      "source": [
        "# Write the code to perform the inference on this network and provide the output as extractive summary to the input paragraph. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PloH4jR0oolm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1rCUjOuJq2cZGp2u9ZYRTgQIAMyDVLFgr",
      "authorship_tag": "ABX9TyOtfs71vF4/8xPEsyjy5Bdp"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}